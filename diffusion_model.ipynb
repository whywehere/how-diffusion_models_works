{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from diffusion_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat=256, n_cfeat=10, height=28):\n",
    "        \"\"\"条件化的U-Net架构，支持时间步和上下文特征注入\n",
    "        Args:\n",
    "            in_channels: 输入图像的通道数 (如灰度图为1，RGB为3)\n",
    "            n_feat: 基础特征通道数 (默认256)\n",
    "            n_cfeat: 上下文特征的维度 (默认10)\n",
    "            height: 输入图像的高度 (必须能被4整除，如28/24/20/16等)\n",
    "        \"\"\"\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        # 初始化基础参数\n",
    "        self.in_channels = in_channels  # 输入通道数\n",
    "        self.n_feat = n_feat            # 特征通道基数\n",
    "        self.n_cfeat = n_cfeat          # 上下文特征维度\n",
    "        self.h = height                 # 图像尺寸（假设h == w）\n",
    "\n",
    "        # 初始卷积块（含残差连接）\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        # 下采样路径（两个层级）\n",
    "        self.down1 = UnetDown(n_feat, n_feat)     # 下采样1：[B, 256, H/2, W/2]\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat) # 下采样2：[B, 512, H/4, W/4]\n",
    "\n",
    "        # 特征向量化层（替代原设计的AvgPool2d(7)）\n",
    "        self.to_vec = nn.Sequential(\n",
    "            nn.AvgPool2d((4)),   # 将4x4特征图池化为1x1 \n",
    "            nn.GELU()            # 高斯误差线性单元激活\n",
    "        )\n",
    "\n",
    "        # 时间步与上下文特征嵌入层\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)       # 时间嵌入1（高维：2*n_feat）\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)       # 时间嵌入2（低维：1*n_feat）\n",
    "        self.contextembed1 = EmbedFC(n_cfeat, 2*n_feat) # 上下文嵌入1（高维）\n",
    "        self.contextembed2 = EmbedFC(n_cfeat, 1*n_feat) # 上下文嵌入2（低维）\n",
    "\n",
    "        # 上采样路径（三个层级）\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*n_feat, 2*n_feat, self.h//4, self.h//4), # 转置卷积上采样4倍\n",
    "            nn.GroupNorm(8, 2*n_feat),  # 分组归一化（8组）                        \n",
    "            nn.ReLU(),                   # 非线性激活\n",
    "        )\n",
    "        self.up1 = UnetUp(4*n_feat, n_feat)  # 上采样1：处理拼接后的4*n_feat通道\n",
    "        self.up2 = UnetUp(2*n_feat, n_feat)  # 上采样2：处理拼接后的2*n_feat通道\n",
    "\n",
    "        # 最终输出层\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2*n_feat, n_feat, 3, padding=1),  # 降维卷积（3x3核）\n",
    "            nn.GroupNorm(8, n_feat),     # 分组归一化\n",
    "            nn.ReLU(),                    # 非线性激活\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, padding=1), # 输出层（与输入通道对齐）\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t, c=None):\n",
    "        \"\"\"\n",
    "        前向传播过程\n",
    "        Args:\n",
    "            x : (B, C, H, W)  输入噪声图像\n",
    "            t : (B, 1)        扩散时间步 \n",
    "            c : (B, n_cfeat)  上下文标签（可为None）\n",
    "        Returns:\n",
    "            out: (B, C, H, W) 预测的去噪图像\n",
    "        特征维度变化\n",
    "            输入图像: [B, 1, 28, 28]\n",
    "            init_conv → [B, 256, 28, 28]\n",
    "            down1    → [B, 256, 14, 14]\n",
    "            down2    → [B, 512, 7, 7]\n",
    "            to_vec   → [B, 512, 1, 1]\n",
    "            up0      → [B, 512, 4, 4]\n",
    "            up1      → [B, 256, 14, 14] \n",
    "            up2      → [B, 256, 28, 28]\n",
    "            输出     → [B, 1, 28, 28]\n",
    "        \"\"\"\n",
    "        # 初始化上下文（若未提供）\n",
    "        if c is None:\n",
    "            c = torch.zeros(x.shape[0], self.n_cfeat).to(x)\n",
    "            \n",
    "        # 初始卷积\n",
    "        x = self.init_conv(x)          # [B, 256, 28, 28]\n",
    "        \n",
    "        # 下采样路径\n",
    "        down1 = self.down1(x)          # [B, 256, 14, 14]\n",
    "        down2 = self.down2(down1)      # [B, 512, 7, 7]\n",
    "        \n",
    "        # 特征向量化\n",
    "        hiddenvec = self.to_vec(down2) # [B, 512, 1, 1]\n",
    "        \n",
    "        # 生成条件嵌入（调整形状为[B, C, 1, 1]）\n",
    "        cemb1 = self.contextembed1(c).view(-1, 2*self.n_feat, 1, 1)  # 高维上下文嵌入\n",
    "        temb1 = self.timeembed1(t).view(-1, 2*self.n_feat, 1, 1)     # 高维时间嵌入\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)    # 低维上下文嵌入\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)       # 低维时间嵌入\n",
    "\n",
    "        # 上采样路径（带条件注入）\n",
    "        up1 = self.up0(hiddenvec)                     # [B, 512, 4, 4] 初始上采样\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)      # 高维条件融合 + 跳跃连接 [B, 256, 14, 14]\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)      # 低维条件融合 + 跳跃连接 [B, 256, 28, 28]\n",
    "        \n",
    "        # 最终输出（拼接原始输入）\n",
    "        out = self.out(torch.cat((up3, x), 1))        # [B, C, 28, 28]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# diffusion hyperparameters\n",
    "timesteps = 500\n",
    "beta1 = 1e-4\n",
    "beta2 = 0.02\n",
    "\n",
    "# network hyperparameters\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else torch.device('cpu'))\n",
    "n_feat = 64 # 64 hidden dimension feature\n",
    "n_cfeat = 5 # context vector is of size 5\n",
    "height = 16 # 16x16 image\n",
    "save_dir = './weights/'\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 100\n",
    "n_epoch = 32\n",
    "lrate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct DDPM noise schedule\n",
    "b_t = (beta2 - beta1) * torch.linspace(0, 1, timesteps + 1, device=device) + beta1\n",
    "a_t = 1 - b_t\n",
    "\n",
    "ab_t = torch.cumsum(a_t.log(), dim=0).exp()    # 累乘\n",
    "ab_t[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, n_feat=n_feat, n_cfeat=n_cfeat, height=height).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sprite shape: (89400, 16, 16, 3)\n",
      "labels shape: (89400, 5)\n"
     ]
    }
   ],
   "source": [
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset(\"./sprites_1788_16x16.npy\", \"./sprite_labels_nc_1788_16x16.npy\", transform, null_context=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加噪公式\n",
    "def perturb_input(x, t, noise):\n",
    "    return ab_t.sqrt()[t, None, None, None] * x + (1 - ab_t[t, None, None, None]) * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training without context code\n",
    "\n",
    "# set into train mode\n",
    "nn_model.train()\n",
    "\n",
    "for ep in range(n_epoch):\n",
    "    print(f'epoch {ep}')\n",
    "    \n",
    "    # linearly decay learning rate\n",
    "    optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, mininterval=2 )\n",
    "    for x, _ in pbar:   # x: images\n",
    "        optim.zero_grad()\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perturb data\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, timesteps + 1, (x.shape[0],)).to(device) \n",
    "        x_pert = perturb_input(x, t, noise)\n",
    "        \n",
    "        # use network to recover noise\n",
    "        pred_noise = nn_model(x_pert, t / timesteps)\n",
    "        \n",
    "        # loss is mean squared error between the predicted and true noise\n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "        loss.backward()\n",
    "        \n",
    "        optim.step()\n",
    "\n",
    "    # save model periodically\n",
    "    if ep%4==0 or ep == int(n_epoch-1):\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        torch.save(nn_model.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "        print('saved model at ' + save_dir + f\"model_{ep}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去噪公式\n",
    "def denoise_add_noise(x, t, pred_noise, z=None):\n",
    "    if z is None:\n",
    "        z = torch.randn_like(x)\n",
    "    noise = b_t.sqrt()[t] * z\n",
    "    mean = (x - pred_noise * ((1 - a_t[t]) / (1 - ab_t[t]).sqrt())) / a_t[t].sqrt()\n",
    "    return mean + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using standard algorithm\n",
    "@torch.no_grad()\n",
    "def sample_ddpm(n_sample, save_rate=20):\n",
    "    # x_T ~ N(0, 1), 随机生产初始噪声x_T\n",
    "    samples = torch.randn(n_sample, 3, height, height).to(device)  \n",
    "\n",
    "    # 存储中间结果\n",
    "    intermediate = [] \n",
    "    for i in range(timesteps, 0, -1):\n",
    "        print(f'sampling timestep {i:3d}', end='\\r')\n",
    "\n",
    "        # reshape time tensor\n",
    "        t = torch.tensor([i / timesteps])[:, None, None, None].to(device)\n",
    "\n",
    "        # sample some random noise to inject back in. For i = 1, don't add back in noise\n",
    "        z = torch.randn_like(samples) if i > 1 else 0\n",
    "\n",
    "        eps = nn_model(samples, t)    # predict noise e_(x_t,t)\n",
    "        samples = denoise_add_noise(samples, i, eps, z)\n",
    "        if i % save_rate ==0 or i==timesteps or i<8:\n",
    "            intermediate.append(samples.detach().cpu().numpy())\n",
    "\n",
    "    intermediate = np.stack(intermediate)\n",
    "    return samples, intermediate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize samples\n",
    "plt.clf()\n",
    "samples, intermediate_ddpm = sample_ddpm(32)\n",
    "animation_ddpm = plot_sample(intermediate_ddpm,32,4,save_dir, \"ani_run\", None, save=False)\n",
    "HTML(animation_ddpm.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
